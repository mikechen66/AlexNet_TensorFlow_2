# AlexNet Model

"""
AlexNet, Krizhevsky, Alex, Ilya Sutskever and Geoffrey E. Hinton, 2012
https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf

Michael Guerzhoy and Davi Frossard, 2017
AlexNet implementation in TensorFlow, with weights Details: 
#http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/

Stanford c231n 
https://cs231n.github.io/convolutional-networks/#conv

The script is the realizatio of object oriented style of AlexNet. The construction method of AlexNet 
includes three parameters, including self, input_shape, num_classes, of which, input_shape works as a 
placeholder. In consideration of the actual inputs has at least 2 classes, we set the class nunber from 
2 t0 1000. 

According to the formula of Stanford cs231, W_output=(W-F+2P)S+1. W,F,P,S are input width, filter width, 
padding size and stride respectively. It is apparent result of H_output=W_output since we requires square 
size of filters. 
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout
import numpy as np

# -img_rows, img_cols = 227, 227
# -input_shape = (img_rows, img_cols, 3)
# Generate the classes of random integer from 2 to 1000
num_classes = np.random.randint(2, 1000)

# Define the AlexNet Model 

class AlexNet(Sequential):
    def __init__(self, input_shape, num_classes):
        super().__init__()

        # No.1 Convolutional Layer: (227- 11 + 2 * 0) / 4 + 1 = 55 
        self.add(Conv2D(filters=96, kernel_size=(11,11), strides=4,
                        padding='valid', activation='relu',
                        input_shape=input_shape, kernel_initializer='he_normal'))
        # Max Pooling: (55- 3 + 2 * 0) / 2 + 1 = 27
        self.add(MaxPooling2D(pool_size=(3,3), strides=(2,2),
                              padding='valid', data_format=None))
        
        # No.2 Conv Layer: (27- 5 + 2 * 2) / 1 + 1 = 27
        self.add(Conv2D(filters=256, kernel_size=(5,5), strides=1,
                        padding='same', activation='relu',
                        kernel_initializer='he_normal'))
        # Max Pooling: (27-  3 + 2 * 0) / 2 + 1 = 13
        self.add(MaxPooling2D(pool_size=(3,3), strides=(2,2),
                              padding='valid', data_format=None))
        
        # No.3 Conv Layer: (13 - 3 + 2 * 1) / 1 + 1 = 13
        self.add(Conv2D(filters=384, kernel_size=(3,3), strides=1,
                        padding='same', activation='relu',
                        kernel_initializer='he_normal'))

        # No.4 Conv Layer: (13 - 3 + 2 * 1)/1 + 1 = 13
        self.add(Conv2D(filters=384, kernel_size=(3,3), strides=1,
                 padding='same', activation='relu',
                 kernel_initializer='he_normal'))

        # No.5 Conv Layer: (13 - 3 + 2 * 1)/1 + 1 = 13
        self.add(Conv2D(filters=256, kernel_size=(3,3), strides=1,
                 padding='same', activation='relu',
                 kernel_initializer='he_normal')) 
        # Max Pooling: (13 - 3 + 2 * 0) / 2 + 1 =  6
        self.add(MaxPooling2D(pool_size=(3,3), strides=(2,2),
                              padding='valid', data_format=None))

        # Flatten the three dimensions of 256 x 6 x 6 into one dimension of 9216.
        self.add(Flatten())

        # No.6 FC Layer 
        self.add(Dense(4096, activation='relu'))
        # Add Dropout
        self.add(Dropout(0.5))

        # No.7 FC Layer
        self.add(Dense(4096, activation='relu'))
        # Add Dropout 
        self.add(Dropout(0.5))

        # No.8 FC Layer
        self.add(Dense(1000, activation='relu'))
        self.add(Dense(num_classes, activation='softmax'))

        self.compile(optimizer=tf.keras.optimizers.Adam(0.001),
                     loss='categorical_crossentropy',
                     metrics=['accuracy'])

# Call the AlexNet model
model = AlexNet((227, 227, 3), num_classes)

# show the full model structure of AlexNet 
model.summary()




        
